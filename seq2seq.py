# -*- coding: utf-8 -*-
"""Seq2Seq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ypTjDkf3hcCNAsNU4qzroXgO7gF99kGL
"""

from io import open
import unicodedata
import string
import re
import random
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

sos_token = 0
eos_token = 1

class lang:
  def __init__(self,name):
    self.name = name
    self.word2index = {}
    self.word2count = {}
    self.index2word = {0:"sos",1:"eos"}
    self.nowords = 2 
  def addsentence(self,sentence):
    for word in sentence.split(" "):
      self.addword(word)
  def addword(self,word):
    if word not in self.word2index:
      self.word2count[word]=1
      self.word2index[word]=self.nowords
      self.index2word[self.nowords] = word
      self.nowords+=1
    else:
      self.word2count[word]+=1

def unicode2ascii(s):
  return ''.join(
      c for c in unicodedata.normalize('NFD',s)
      if unicodedata.category(c)!= 'Mn'
  )
def normalize(s):
  s=unicode2ascii(s.lower().strip())
  s = re.sub(r"([.!?])", r" \1", s)
  s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
  return s

def readlangs(lang1,lang2,reverse = False):

  lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\n')

  pairs = [[normalize(s) for s in l.split('\t')] for l in lines]
  if reverse:
    pairs = [list(reversed(p)) for p in pairs]
    input_lang = lang(lang2)
    output_lang = lang(lang1)
  else:
    input_lang = lang(lang1)
    output_lang = lang(lang2)

  return input_lang,output_lang,pairs

MAX_LENGTH = 10
eng_prefixes = (
    "i am ", "i m ",
    "he is", "he s ",
    "she is", "she s ",
    "you are", "you re ",
    "we are", "we re ",
    "they are", "they re "
)
def filterpair(p):
  return len(p[0].split(' '))<MAX_LENGTH and len(p[1].split(' '))<MAX_LENGTH and p[1].startswith(eng_prefixes)
def filteredpairs(pairs):
  return [pair for pair in pairs if filterpair(pair)]

def prepdata(lang1,lang2,reverse=False):
  input_lang,output_lang,pairs = readlangs(lang1,lang2,reverse)
  pairs = filteredpairs(pairs)
  for pair in pairs:
    input_lang.addsentence(pair[0])
    output_lang.addsentence(pair[1])
  return input_lang,output_lang,pairs
input_lang,output_lang,pairs = prepdata('eng','fra',True)

print(len(pairs))
print(output_lang.nowords)
print(random.choice(pairs))

class encoder(nn.Module):
  def __init__(self,input_size,hidden_size):
    super(encoder,self).__init__()
    self.hidden_size = hidden_size
    self.embedding = nn.Embedding(input_size,hidden_size)
    self.gru = nn.GRU(hidden_size,hidden_size)
    
  def forward(self,input,hidden):
    embedded = self.embedding(input)
    embedded = embedded.view(1,1,-1)
    output = embedded
    output,hidden = self.gru(output,hidden)
    return output,hidden

  def inithidden(self):
    return torch.zeros(1,1,self.hidden_size,device=device)

class decoder(nn.Module):
  def __init__(self,hidden_size,output_size,dropout=0.1,max_length=MAX_LENGTH):
    super(decoder,self).__init__()
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.dropout = dropout
    self.max_length = max_length
    self.embedding = nn.Embedding(self.output_size,self.hidden_size)
    self.attn = nn.Linear((self.hidden_size)*2,self.max_length)
    self.attn_combine = nn.Linear((self.hidden_size)*2, self.hidden_size)
    self.dropout = nn.Dropout(self.dropout)
    self.gru = nn.GRU(self.hidden_size,self.hidden_size)
    self.out = nn.Linear(self.hidden_size,self.output_size)
  
  def forward(self,input,hidden,encoder_outputs):
    embedded = self.embedding(input)
      
    embedded = embedded.view(1,1,-1)
    embedded = self.dropout(embedded)
    attn_w = F.softmax(self.attn(torch.cat((embedded[0],hidden[0]),1)),dim=1)
    attn_applied = torch.bmm(attn_w.unsqueeze(0),encoder_outputs.unsqueeze(0))
    output = torch.cat((embedded[0],attn_applied[0]),1)
    output = self.attn_combine(output).unsqueeze(0)
    output = F.relu(output)
    output,hidden=self.gru(output,hidden)
    output = F.log_softmax(self.out(output[0]),dim=1)
    return output,hidden,attn_w
  
  def inithidden(self):
    return torch.zeros(1,1,self.hidden_size,device=device)



def IndicesFromSentences(lang,sentence):
  return [lang.word2index[word] for word in sentence.split(' ')]
  
def TensorFromSentences(lang,sentence):

  indices = IndicesFromSentences(lang,sentence)
  indices.append(eos_token)
  return torch.tensor(indices, dtype=torch.long, device=device).view(-1,1)

def tensorfrompair(pair):
  input_tensor = TensorFromSentences(input_lang,pair[0])
  target_tensor = TensorFromSentences(output_lang,pair[1])
  return (input_tensor,target_tensor)



teacher_forcing_ratio = 0.5 #Prob of using the target output as input for decoder in the next layer instead of using the prediction of the previous layer

def train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion,max_length = MAX_LENGTH):
   
  encoder_hidden = encoder.inithidden()
  encoder_optimizer.zero_grad()
  decoder_optimizer.zero_grad()
  input_length = input_tensor.size(0)
  target_length = target_tensor.size(0)
  encoder_outputs = torch.zeros(max_length,encoder.hidden_size,device=device)
  loss=0
  for ei in range(input_length):
    encoder_output,encoder_hidden = encoder(input_tensor[ei],encoder_hidden)
    encoder_outputs[ei] = encoder_output[0,0]

  decoder_input = torch.tensor([[sos_token]],device = device)
  decoder_hidden = encoder_hidden

  use_teacher_forcing = True if random.random()<teacher_forcing_ratio else False
  if use_teacher_forcing:
    for di in range(target_length):
      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input,decoder_hidden,encoder_outputs)
      topv,topi = decoder_output.data.topk(1)
      #print(topi)
      loss+=criterion(decoder_output,target_tensor[di])
      decoder_input = target_tensor[di]
  else:
    for di in range(target_length):
      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input,decoder_hidden,encoder_outputs)
      topv, topi = decoder_output.topk(1)
      decoder_input = topi.squeeze().detach()
      loss+=criterion(decoder_output,target_tensor[di])
      if decoder_input.item()==eos_token:
        break
  loss.backward()
  encoder_optimizer.step()
  decoder_optimizer.step()
  return loss.item()/target_length

import matplotlib.pyplot as plt
plt.switch_backend('agg')
import matplotlib.ticker as ticker
import numpy as np


def showPlot(points):
    plt.figure()
    fig, ax = plt.subplots()
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)
    plt.show()

def trainiters(encoder,decoder,n_iter,print_every=5000,plot_every=100,lr=0.01):
  plot_losses=[]
  print_loss_total = 0
  plot_loss_total = 0
  encoder_optimizer = optim.SGD(encoder.parameters(),lr=lr)
  decoder_optimizer = optim.SGD(decoder.parameters(),lr=lr)
  training_pairs = [tensorfrompair(random.choice(pairs)) for i in range(n_iter)]
  criterion = nn.NLLLoss()

  for i in range(n_iter):

    training_pair = training_pairs[i]
    input_tensor = training_pair[0]
    target_tensor = training_pair[1]
    loss = train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion)
    print_loss_total+=loss
    plot_loss_total+=loss
    if i % print_every==0:
      print_loss_avg = print_loss_total/print_every
      print_loss_total = 0
      print(print_loss_avg)
    if i % plot_every == 0:
      plot_loss_avg = plot_loss_total / plot_every
      plot_losses.append(plot_loss_avg)
      plot_loss_total = 0
  showPlot(plot_losses)

hidden_size = 256
encoder1 = encoder(input_lang.nowords, hidden_size).to(device)
attn_decoder1 = decoder(hidden_size, output_lang.nowords, dropout=0.1).to(device)

trainiters(encoder1, attn_decoder1, 75000, print_every=100)

def evaluate(encoder,decoder,sentence,max_length = MAX_LENGTH):
  with torch.no_grad():
    input_tensor = TensorFromSentences(input_lang,sentence)
    input_length = input_tensor.size()[0]
    #print(input_length)
    print(input_tensor)
    #print(eos_token)
    encoder_hidden = encoder.inithidden()
    encoder_outputs = torch.zeros(max_length,encoder.hidden_size,device=device)
    for ei in range(input_length):
      encoder_output,encoder_hidden = encoder(input_tensor[ei],encoder_hidden)
      encoder_outputs[ei]+=encoder_output[0,0]
    decoder_input = torch.tensor([[sos_token]],device=device)
    decoder_hidden = encoder_hidden
    decoded_words = []
    decoder_attentions = torch.zeros(max_length,max_length)
    for di in range(max_length):
      decoder_output,decoder_hidden,decoder_attention = decoder(decoder_input,decoder_hidden,encoder_outputs)
      decoder_attentions[di] = decoder_attention.data
      topv, topi = decoder_output.data.topk(1)
      print(topi)
      if topi.item()==eos_token:
        decoded_words.append('<eos>')
        break
      else:
        decoded_words.append(output_lang.index2word[topi.item()])

      decoded_input = topi.squeeze().detach()
    return decoded_words

def test(input_sequence):
  answers =  evaluate(encoder1,attn_decoder1,input_sequence)
  print("output = ",' '.join(answers))
test("elle a cinq ans de moins que moi .")

def evaluateRandomly(encoder, decoder, n=10):
    for i in range(n):
        pair = random.choice(pairs)
        print('>', pair[0])
        print('=', pair[1])
        output_words = evaluate(encoder, decoder, pair[0])
        output_sentence = ' '.join(output_words)
        print('<', output_sentence)
        print('')

evaluateRandomly(encoder1, attn_decoder1)