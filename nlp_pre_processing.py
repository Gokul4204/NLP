# -*- coding: utf-8 -*-
"""NLP_pre-processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VzPGVTFTcJ3LEtFpoQ11fvL3MQ1je0d-
"""

from IPython.display import clear_output
!pip install pyprind
!pip install unidecode
!pip install contractions
!pip install nltk
clear_output()

import pandas as pd
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.zip
!unzip /content/spambase.zip
!rm -rf spambase.zip
!rm /content/spambase.DOCUMENTATION
!rm /content/spambase.names
clear_output()

cols = ["makes", "address", "all", "3d", "our", "over", "remove", "internet", "order", "mail", "receive", "will", "people", 
                "report", "addresses", "free", "business", "email", "you", "credit", "your", "font", "000", "money", "hp", "hpl", 
                "george", "650", "lab", "labs", "telnet", "857", "data", "415", "85", "technology", "1999", "parts", "pm", "direct", 
                "cs", "meeting", "original", "project", "re", "edu", "table", "conference", ";", "(", "[", "!", "$", "#", "CAPS_AVG", 
                "CAPS_LONGEST", "CAPS_TOTAL", "SPAM"]

data = pd.read_csv("/content/spambase.data", names=cols)

print(data)

cols1 = []
cols2 = []
for i in range(len(cols)):
  cols1.append(cols[i])
  cols2.append(cols[i])

#data already tokenized
lemmatizer = WordNetLemmatizer()
for i in range(48):
  if(cols1[i].isalpha()):
    cols1[i] = lemmatizer.lemmatize(cols1[i])

ps = PorterStemmer()
for i in range(48):
  if(cols2[i].isalpha()):
    cols2[i] = ps.stem(cols2[i])
print(cols2)
print(cols1)
print(cols)